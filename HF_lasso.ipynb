{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF factors\n",
    "def get_realvar(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realskew(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).skew().fillna(0)\n",
    "\n",
    "def get_realkurtosis(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).kurt().fillna(0)\n",
    "\n",
    "def get_realupvar(df, lookback_len, lookback_shift):\n",
    "    df['return_up'] = df['return'][df['return'] > 0]\n",
    "    df['return_up'] = df['return_up'].fillna(0)\n",
    "    return df['return_up'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realdownvar(df, lookback_len, lookback_shift):\n",
    "    df['return_down'] = df['return'][df['return'] < 0]\n",
    "    df['return_down'] = df['return_down'].fillna(0)\n",
    "    return df['return_down'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_ratio_upvar(df, lookback_len, lookback_shift):\n",
    "    return get_realupvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_ratio_downvar(df, lookback_len, lookback_shift):\n",
    "    return get_realdownvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_trendratio(df, lookback_len, lookback_shift):\n",
    "    abs_price_diff = abs(df['price'].diff()).fillna(0)\n",
    "    abs_price_diff_sum = abs_price_diff.shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "    trend_ratio = (df['price']-df['price'].shift(lookback_len)).shift(lookback_shift) / abs_price_diff_sum\n",
    "    return trend_ratio.replace(np.inf, 0).fillna(0)\n",
    "\n",
    "def get_windowreturn(df, lookback_len, lookback_shift):\n",
    "    return np.exp((np.log(df['return']+1)).shift(lookback_shift).rolling(lookback_len).sum())-1\n",
    "\n",
    "def get_minreturn(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).min().fillna(0)\n",
    "\n",
    "def calculate_mdd(series):\n",
    "    max_price = np.maximum.accumulate(series)\n",
    "    drawdown = (max_price - series) / max_price\n",
    "    return np.max(drawdown)\n",
    "\n",
    "def get_mdd(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).apply(lambda x: calculate_mdd(x), raw=True)\n",
    "\n",
    "def get_corrVP_price(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "# def get_corrVP_avg(df, lookback_len, lookback_shift):\n",
    "#     return df['price'].fillna(method='ffill').shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVP_mid(df, lookback_len, lookback_shift):\n",
    "    return df['mid'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVR(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_Amihud(df, lookback_len, lookback_shift):\n",
    "    abs_return = abs(df['return'].diff()).fillna(0)\n",
    "    sum_abs_return = abs_return.shift(lookback_shift).rolling(lookback_len).sum()\n",
    "    return (1 / (lookback_len) * sum_abs_return / df['amount'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_BAspread(df, lookback_len, lookback_shift):\n",
    "    bidsum = df[\"b1\"]*df[\"b1_v\"]+0.8*df[\"b2\"]*df[\"b2_v\"]+0.6*df[\"b3\"]*df[\"b3_v\"]+0.4*df[\"b4\"]*df[\"b4_v\"]+0.2*df[\"b5\"]*df[\"b5_v\"]\n",
    "    asksum = df[\"a1\"]*df[\"a1_v\"]+0.8*df[\"a2\"]*df[\"a2_v\"]+0.6*df[\"a3\"]*df[\"a3_v\"]+0.4*df[\"a4\"]*df[\"a4_v\"]+0.2*df[\"a5\"]*df[\"a5_v\"]\n",
    "    df[\"spread\"] = (bidsum - asksum) / (bidsum + asksum)\n",
    "    return df[\"spread\"].shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def delta_V_A(a1, a1_v):\n",
    "    # a1 and a1_v are ndarrays\n",
    "    diff = a1[-1] - a1[0]\n",
    "    if diff < 0:\n",
    "        return a1_v[-1]\n",
    "    elif diff == 0:\n",
    "        return a1_v[-1] - a1_v[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def delta_V_B(b1, b1_v):\n",
    "    # b1 and b1_v are ndarrays\n",
    "    diff = b1[-1] - b1[0]\n",
    "    if diff < 0:\n",
    "        return 0\n",
    "    elif diff == 0:\n",
    "        return b1_v[-1] - b1_v[0]\n",
    "    else:\n",
    "        return b1_v[-1]\n",
    "    \n",
    "def get_VOI(df, lookback_len, lookback_shift):\n",
    "    delta_Va = np.zeros_like(df['a1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['a1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['a1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Va'] = delta_Va\n",
    "\n",
    "    delta_Vb = np.zeros_like(df['b1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['b1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['b1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Vb'] = delta_Vb\n",
    "\n",
    "    df['ori_VOI'] = df['delta_Vb'] - df['delta_Va']\n",
    "    df['ori_VOI'] = df['ori_VOI'].fillna(0)\n",
    "    df = df.drop(columns=['delta_Va'])\n",
    "    df = df.drop(columns=['delta_Vb'])\n",
    "\n",
    "    mean = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).mean()\n",
    "    std = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).std()\n",
    "    return ((df['ori_VOI'] - mean)/std).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def get_BAspread_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2'] - df['a2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3'] - df['a3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4'] - df['a4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5'] - df['a5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_midprice_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return ((df['b1']+df['a1'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_midprice_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return ((df['b2']+df['a2'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_midprice_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return ((df['b3']+df['a3'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_midprice_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return ((df['b4']+df['a4'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_midprice_5_mean(df, lookback_len, lookback_shift):\n",
    "#     return ((df['b5']+df['a5'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_ap_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a2'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_ap_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a3'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_ap_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a4'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_ap_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a5'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_bp_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b1'] - df['b2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_bp_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b1'] - df['b3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_bp_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b1'] - df['b4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_bp_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b1'] - df['b5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_ap_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['a2'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_ap_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['a3'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_ap_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['a4'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_ap_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['a5'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_bp_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['b1'] - df['b2'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_bp_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['b1'] - df['b3'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_bp_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['b1'] - df['b4'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_abs_bp_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (abs(df['b1'] - df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1'] + df['a2'] + df['a3'] + df['a4'] + df['a5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1'] + df['b2'] + df['b3'] + df['b4'] + df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_av_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1_v'] + df['a2_v'] + df['a3_v'] + df['a4_v'] + df['a5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bv_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1_v'] + df['b2_v'] + df['b3_v'] + df['b4_v'] + df['b5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_sum_ap_diff_mean(df, lookback_len, lookback_shift):\n",
    "#     return get_ap_diff_1_mean(df, lookback_len, lookback_shift) + get_ap_diff_2_mean(df, lookback_len, lookback_shift) + get_ap_diff_3_mean(df, lookback_len, lookback_shift) + get_ap_diff_4_mean(df, lookback_len, lookback_shift)\n",
    "# def get_sum_bp_diff_mean(df, lookback_len, lookback_shift):\n",
    "#     return get_bp_diff_1_mean(df, lookback_len, lookback_shift) + get_bp_diff_2_mean(df, lookback_len, lookback_shift) + get_bp_diff_3_mean(df, lookback_len, lookback_shift) + get_bp_diff_4_mean(df, lookback_len, lookback_shift)\n",
    "\n",
    "# def get_deriv_ap_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a1'] - df['a1'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_ap_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a2'] - df['a2'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_ap_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a3'] - df['a3'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_ap_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a4'] - df['a4'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_ap_5_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['a5'] - df['a5'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_bp_1_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b1'] - df['b1'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_bp_2_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b2'] - df['b2'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_bp_3_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b3'] - df['b3'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_bp_4_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b4'] - df['b4'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "# def get_deriv_bp_5_mean(df, lookback_len, lookback_shift):\n",
    "#     return (df['b5'] - df['b5'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a1_v'] - df['a1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2_v'] - df['a2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3_v'] - df['a3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4_v'] - df['a4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5_v'] - df['a5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1_v'] - df['b1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2_v'] - df['b2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3_v'] - df['b3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4_v'] - df['b4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5_v'] - df['b5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_depth_price_range(df, lookback_len, lookback_shift):\n",
    "    return (df['a1'].shift(lookback_shift).rolling(lookback_len).max() / df['a1'].shift(lookback_shift).rolling(lookback_len).min() - 1).fillna(0)\n",
    "\n",
    "import numba as nb\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def age(prices):\n",
    "    last_value = prices[-1]\n",
    "    age = 0\n",
    "    for i in range(2, len(prices)):\n",
    "        if prices[-i] != last_value:\n",
    "            return age\n",
    "        age += 1\n",
    "    return age\n",
    "\n",
    "def get_BAage(df, lookback_len, lookback_shift):\n",
    "    return df['b1'].shift(lookback_shift).rolling(lookback_len).apply(age, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_cofi(df, lookback_len, lookback_shift):\n",
    "    a = df['b1_v']*np.where(df['b1'].diff()>=0, 1, 0)\n",
    "    b = df['b1_v'].shift()*np.where(df['b1'].diff()<=0, 1, 0)\n",
    "    c = df['a1_v']*np.where(df['a1'].diff()>=0, 1, 0)\n",
    "    d = df['a1_v'].shift()*np.where(df['a1'].diff()<=0, 1, 0)\n",
    "    return (a-b-c+d).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_bp_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['b1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "def get_ap_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['a1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def first_location_of_maximum(x):\n",
    "    max_value = max(x)\n",
    "    for loc in range(len(x)):\n",
    "        if x[loc] == max_value:\n",
    "            return loc + 1\n",
    "        \n",
    "def get_price_idxmax(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(first_location_of_maximum, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def mean_second_derivative_centra(x):\n",
    "    sum_value = 0\n",
    "    for i in range(len(x)-5):\n",
    "        sum_value += (x[i+5]-2*x[i+3]+x[i])/2\n",
    "    return sum_value/(2*(len(x)-5))\n",
    "\n",
    "def get_center_deri_two(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(mean_second_derivative_centra, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_quasi(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].diff(1).abs().shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "\n",
    "def get_weighted_price_to_mid(df, lookback_len, lookback_shift):\n",
    "    avs = df[['a1_v', 'a2_v', 'a3_v', 'a4_v', 'a5_v']].values\n",
    "    bvs = df[['b1_v', 'b2_v', 'b3_v', 'b4_v', 'b5_v']].values\n",
    "    aps = df[['a1', 'a2', 'a3', 'a4', 'a5']].values\n",
    "    bps = df[['b1', 'b2', 'b3', 'b4', 'b5']].values\n",
    "    return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of factors: 414\n"
     ]
    }
   ],
   "source": [
    "lookback_len = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "lookback_shift = 0\n",
    "\n",
    "for i, length in enumerate(lookback_len):\n",
    "    if i == 0:\n",
    "        functions = {f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')}\n",
    "    else:\n",
    "        functions.update({f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')})\n",
    "        \n",
    "\n",
    "print('numbers of factors:', len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.rename(columns={'Unnamed: 0': 'tick'})\n",
    "    df['lastPx'] = df['lastPx'].fillna(method='ffill')\n",
    "    df['BP1'] = df['BP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    df['SP1'] = df['SP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    # assert df['avg_price'].isna().sum() == 0\n",
    "    df = df[df['BP1'] != 0]\n",
    "    df = df[df['SP1'] != 0]\n",
    "    df['mid'] = (df['BP1'] + df['SP1']) / 2\n",
    "    df['diff_v'] = (df['volume'] - df['volume'].shift(1)).fillna(0)\n",
    "    df['return'] = (df['mid'] / df['mid'].shift(1) - 1).fillna(0)\n",
    "    df = df.rename(columns={'lastPx': 'price', 'BP1':'b1', 'BP2':'b2', 'BP3':'b3', 'BP4':'b4', 'BP5':'b5', \n",
    "                            'SP1':'a1', 'SP2':'a2', 'SP3':'a3', 'SP4':'a4', 'SP5':'a5',\n",
    "                            'BV1':'b1_v', 'BV2':'b2_v', 'BV3':'b3_v', 'BV4':'b4_v', 'BV5':'b5_v',\n",
    "                            'SV1':'a1_v', 'SV2':'a2_v', 'SV3':'a3_v', 'SV4':'a4_v', 'SV5':'a5_v',\n",
    "                            'volume': 'volume_sum', 'diff_v': 'volume'})\n",
    "    df['amount'] = df['price'] * df['volume']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 定义一个函数，用于检查列名是否包含不支持的特殊 JSON 字符\n",
    "def check_col_name(df):\n",
    "    pattern = re.compile(r'[\\s#$.\\[\\]]')\n",
    "    illegal_cols = [col for col in df.columns if pattern.search(col)]\n",
    "    if len(illegal_cols) > 0:\n",
    "        print('以下列名包含不支持的特殊 JSON 字符:')\n",
    "        print(illegal_cols)\n",
    "        # 将不支持的特殊 JSON 字符替换为下划线\n",
    "        new_cols = [re.sub(pattern, '_', col) for col in df.columns]\n",
    "        df.columns = new_cols\n",
    "        print('列名已修改为:')\n",
    "        print(df.columns)\n",
    "    else:\n",
    "        print('列名符合规则，无需修改')\n",
    "\n",
    "# 示例代码\n",
    "import pandas as pd\n",
    "\n",
    "# 检查列名并修改不符合规则的列名\n",
    "check_col_name(df_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = os.listdir('./factors/')\n",
    "for fac in tqdm(factors):\n",
    "    df_prc = pd.read_csv('./factors/'+fac)\n",
    "    df_prc = df_prc.iloc[:, 1:]\n",
    "    # 定义滚动训练的窗口大小\n",
    "    window_size = 5\n",
    "    date_list = df_prc['date'].unique().tolist()\n",
    "\n",
    "    # 定义 LASSO 模型的参数\n",
    "    alpha = 0.0001\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "    # 定义 LASSO 模型\n",
    "    model = Lasso(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "\n",
    "    # 定义用于存储每个 tick 预测结果的列表\n",
    "    r2_list = []\n",
    "    ic_list = []\n",
    "\n",
    "    # 定义用于存储特征重要性的字典\n",
    "    feature_importances = {}\n",
    "    importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "    importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "\n",
    "    # 对于每个滚动窗口\n",
    "    for i in range(window_size, len(date_list)):\n",
    "        # 选择训练数据和目标值\n",
    "        X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "        X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "        y_train = X_train['return'].shift(-1).fillna(0)\n",
    "        X_train = X_train.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "        # 选择测试数据\n",
    "        X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "        X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "        y_test = X_test['return'].shift(-1).fillna(0)\n",
    "        X_test = X_test.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "        # 标准化训练数据和测试数据\n",
    "        feature_cols = X_train.columns\n",
    "        # scaler = StandardScaler()\n",
    "        # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 进行预测\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 计算预测值和真实值之间的R方\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "        # 将R方添加到列表中\n",
    "        r2_list.append(r2)\n",
    "        ic_list.append(corr)\n",
    "\n",
    "        # 记录特征重要性\n",
    "        for feature, coef in zip(feature_cols, model.coef_):\n",
    "            if feature not in feature_importances:\n",
    "                feature_importances[feature] = []\n",
    "            feature_importances[feature].append(coef)\n",
    "\n",
    "        fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "        fi['value'] = model.coef_\n",
    "        fi = fi[fi.value != 0]\n",
    "        fi = fi.sort_values(by='value', ascending=False)\n",
    "        fi = fi.iloc[:10]\n",
    "        importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "        importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "        print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "    # 计算所有天预测的平均 R2 值\n",
    "    r2_mean = np.mean(r2_list)\n",
    "    ic_mean = np.mean(ic_list)\n",
    "\n",
    "    # 输出结果\n",
    "    # print('特征重要性:', feature_importances)\n",
    "    print('平均 R2 值:', r2_mean)\n",
    "    print('平均 IC 值:', ic_mean)\n",
    "    importance_name.to_csv('./lasso_importance_name_'+fac[8:])\n",
    "    importance_value.to_csv('./lasso_importance_value_'+fac[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./data/2330/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '2330':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 1, IC: -0.07455466946751241\n",
      "Total training number: 16, completed: 2, IC: 0.031627987508390756\n",
      "Total training number: 16, completed: 3, IC: 0.03885893340098255\n",
      "Total training number: 16, completed: 4, IC: 0.02930807526607966\n",
      "Total training number: 16, completed: 5, IC: 0.0321110775748027\n",
      "Total training number: 16, completed: 6, IC: 0.09693453397670662\n",
      "Total training number: 16, completed: 7, IC: 0.002769909022618795\n",
      "Total training number: 16, completed: 8, IC: 0.004836262754646683\n",
      "Total training number: 16, completed: 9, IC: 0.027901093817352528\n",
      "Total training number: 16, completed: 10, IC: -0.03768130074435846\n",
      "Total training number: 16, completed: 11, IC: 0.0647230206207126\n",
      "Total training number: 16, completed: 12, IC: 0.05977926187058838\n",
      "Total training number: 16, completed: 13, IC: 0.044169720120260214\n",
      "Total training number: 16, completed: 14, IC: -0.08412887394098445\n",
      "Total training number: 16, completed: 15, IC: 0.06329639034504476\n",
      "Total training number: 16, completed: 16, IC: 0.0552263193330239\n",
      "平均 R2 值: -0.0024928227431614824\n",
      "平均 IC 值: 0.022198608841147174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2330_lasso.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./data/2330/2330_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "window_size = 5\n",
    "date_list = df_prc['date'].unique().tolist()\n",
    "\n",
    "# 定义 LASSO 模型的参数\n",
    "alpha = 0.0001\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "# 定义 LASSO 模型\n",
    "model = Lasso(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "r2_list = []\n",
    "ic_list = []\n",
    "\n",
    "# 定义用于存储特征重要性的字典\n",
    "feature_importances = {}\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "#     importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "#     importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "\n",
    "# 对于每个滚动窗口\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # 选择测试数据\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    # 标准化训练数据和测试数据\n",
    "    feature_cols = X_train.columns\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的R方\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "    # 将R方添加到列表中\n",
    "    r2_list.append(r2)\n",
    "    ic_list.append(corr)\n",
    "\n",
    "    # 记录特征重要性\n",
    "#         for feature, coef in zip(feature_cols, model.coef_):\n",
    "#             if feature not in feature_importances:\n",
    "#                 feature_importances[feature] = []\n",
    "#             feature_importances[feature].append(coef)\n",
    "\n",
    "#         fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "#         fi['value'] = model.coef_\n",
    "#         fi = fi[fi.value != 0]\n",
    "#         fi = fi.sort_values(by='value', ascending=False)\n",
    "#         fi = fi.iloc[:10]\n",
    "#         importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "#         importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "\n",
    "    print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "# 计算所有天预测的平均 R2 值\n",
    "r2_mean = np.mean(r2_list)\n",
    "ic_mean = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "# print('特征重要性:', feature_importances)\n",
    "print('平均 R2 值:', r2_mean)\n",
    "print('平均 IC 值:', ic_mean)\n",
    "pred_value.to_csv('./lasso_pred_2330_'+str(m)+'.csv')\n",
    "joblib.dump(model, '2330_lasso.pkl')\n",
    "#     importance_name.to_csv('./lasso_importance_name_'+file[0:5]+file[8:14]+'.csv')\n",
    "#     importance_value.to_csv('./lasso_importance_value_'+file[0:5]+file[8:14]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./data/0050/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '0050':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 1, IC: 0.11630741714894459\n",
      "Total training number: 16, completed: 2, IC: 0.12363195430692862\n",
      "Total training number: 16, completed: 3, IC: 0.10018354617253669\n",
      "Total training number: 16, completed: 4, IC: 0.11611864698158832\n",
      "Total training number: 16, completed: 5, IC: 0.08601230750235923\n",
      "Total training number: 16, completed: 6, IC: 0.053144986574939705\n",
      "Total training number: 16, completed: 7, IC: 0.17413123312942222\n",
      "Total training number: 16, completed: 8, IC: 0.13098774597163051\n",
      "Total training number: 16, completed: 9, IC: 0.14510955610942314\n",
      "Total training number: 16, completed: 10, IC: 0.04991469912098832\n",
      "Total training number: 16, completed: 11, IC: 0.03526526248588555\n",
      "Total training number: 16, completed: 12, IC: 0.046435476778370836\n",
      "Total training number: 16, completed: 13, IC: 0.13770076967614261\n",
      "Total training number: 16, completed: 14, IC: -0.03453341546394138\n",
      "Total training number: 16, completed: 15, IC: 0.10416698381362957\n",
      "Total training number: 16, completed: 16, IC: 0.17712479496039882\n",
      "平均 R2 值: 0.007362027960232116\n",
      "平均 IC 值: 0.09760637282932796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0050_lasso.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./data/0050/0050_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "window_size = 5\n",
    "date_list = df_prc['date'].unique().tolist()\n",
    "\n",
    "# 定义 LASSO 模型的参数\n",
    "alpha = 0.0001\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "# 定义 LASSO 模型\n",
    "model = Lasso(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "r2_list = []\n",
    "ic_list = []\n",
    "\n",
    "# 定义用于存储特征重要性的字典\n",
    "feature_importances = {}\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "#     importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "#     importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "\n",
    "# 对于每个滚动窗口\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # 选择测试数据\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    # 标准化训练数据和测试数据\n",
    "    feature_cols = X_train.columns\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的R方\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "    # 将R方添加到列表中\n",
    "    r2_list.append(r2)\n",
    "    ic_list.append(corr)\n",
    "\n",
    "    # 记录特征重要性\n",
    "#         for feature, coef in zip(feature_cols, model.coef_):\n",
    "#             if feature not in feature_importances:\n",
    "#                 feature_importances[feature] = []\n",
    "#             feature_importances[feature].append(coef)\n",
    "\n",
    "#         fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "#         fi['value'] = model.coef_\n",
    "#         fi = fi[fi.value != 0]\n",
    "#         fi = fi.sort_values(by='value', ascending=False)\n",
    "#         fi = fi.iloc[:10]\n",
    "#         importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "#         importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "\n",
    "    print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "# 计算所有天预测的平均 R2 值\n",
    "r2_mean = np.mean(r2_list)\n",
    "ic_mean = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "# print('特征重要性:', feature_importances)\n",
    "print('平均 R2 值:', r2_mean)\n",
    "print('平均 IC 值:', ic_mean)\n",
    "pred_value.to_csv('./lasso_pred_0050_'+str(m)+'.csv')\n",
    "joblib.dump(model, '0050_lasso.pkl')\n",
    "\n",
    "#     importance_name.to_csv('./lasso_importance_name_'+file[0:5]+file[8:14]+'.csv')\n",
    "#     importance_value.to_csv('./lasso_importance_value_'+file[0:5]+file[8:14]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./data/2603/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '2603':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/var/folders/rp/8c0pvywd07x0n19dc5yqt8fw0000gn/T/ipykernel_26304/1576437422.py:279: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 1, IC: 0.15745998733676814\n",
      "Total training number: 16, completed: 2, IC: 0.14703912896507848\n",
      "Total training number: 16, completed: 3, IC: 0.08851757215886344\n",
      "Total training number: 16, completed: 4, IC: 0.15596016750516076\n",
      "Total training number: 16, completed: 5, IC: 0.21285020588166467\n",
      "Total training number: 16, completed: 6, IC: 0.0834052725900034\n",
      "Total training number: 16, completed: 7, IC: 0.17985947580456035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e-06, tolerance: 1.092e-06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 8, IC: 0.04721221946248288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e-04, tolerance: 1.456e-06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 9, IC: 0.17861508125269546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.693e-03, tolerance: 1.408e-06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 10, IC: 0.17915238284355628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.332e-03, tolerance: 1.405e-06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 11, IC: 0.06289190444954423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.500e-04, tolerance: 1.162e-06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 12, IC: 0.1279674348538291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzheng/opt/anaconda3/envs/torch1/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.872e-04, tolerance: 8.412e-07\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 13, IC: 0.15698903199674588\n",
      "Total training number: 16, completed: 14, IC: 0.07644703343631336\n",
      "Total training number: 16, completed: 15, IC: 0.10488741524815048\n",
      "Total training number: 16, completed: 16, IC: 0.09291295248624173\n",
      "平均 R2 值: -0.017604348160443428\n",
      "平均 IC 值: 0.12826045414197867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2603_lasso.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./data/2603/2603_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "window_size = 5\n",
    "date_list = df_prc['date'].unique().tolist()\n",
    "\n",
    "# 定义 LASSO 模型的参数\n",
    "alpha = 0.0001\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "# 定义 LASSO 模型\n",
    "model = Lasso(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "r2_list = []\n",
    "ic_list = []\n",
    "\n",
    "# 定义用于存储特征重要性的字典\n",
    "feature_importances = {}\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "#     importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "#     importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "\n",
    "# 对于每个滚动窗口\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # 选择测试数据\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    # 标准化训练数据和测试数据\n",
    "    feature_cols = X_train.columns\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的R方\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "    # 将R方添加到列表中\n",
    "    r2_list.append(r2)\n",
    "    ic_list.append(corr)\n",
    "\n",
    "    # 记录特征重要性\n",
    "#         for feature, coef in zip(feature_cols, model.coef_):\n",
    "#             if feature not in feature_importances:\n",
    "#                 feature_importances[feature] = []\n",
    "#             feature_importances[feature].append(coef)\n",
    "\n",
    "#         fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "#         fi['value'] = model.coef_\n",
    "#         fi = fi[fi.value != 0]\n",
    "#         fi = fi.sort_values(by='value', ascending=False)\n",
    "#         fi = fi.iloc[:10]\n",
    "#         importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "#         importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "\n",
    "    print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "# 计算所有天预测的平均 R2 值\n",
    "r2_mean = np.mean(r2_list)\n",
    "ic_mean = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "# print('特征重要性:', feature_importances)\n",
    "print('平均 R2 值:', r2_mean)\n",
    "print('平均 IC 值:', ic_mean)\n",
    "pred_value.to_csv('./lasso_pred_2603_'+str(m)+'.csv')\n",
    "joblib.dump(model, '2603_lasso.pkl')\n",
    "\n",
    "#     importance_name.to_csv('./lasso_importance_name_'+file[0:5]+file[8:14]+'.csv')\n",
    "#     importance_value.to_csv('./lasso_importance_value_'+file[0:5]+file[8:14]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList = []\n",
    "featureImportance = []\n",
    "\n",
    "for key in feature_importances:\n",
    "    featureList.append(key)\n",
    "    featureImportance.append(np.mean(feature_importances[key]))\n",
    "    \n",
    "## Draw Feature Importance\n",
    "# Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "# Normalize the feature importances\n",
    "dfFeatImp['importance_normalized'] = dfFeatImp['importance'] / dfFeatImp['importance'].sum()\n",
    "\n",
    "# Plot the feature importances in horizontal bar charts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (20, 12))\n",
    "ax = plt.subplot()\n",
    "ax.barh(list(reversed(list(dfFeatImp.index[:30]))), \n",
    "    dfFeatImp['importance_normalized'].head(30), \n",
    "    align = 'center', color = 'mediumseagreen')\n",
    "    \n",
    "# Set the yticks and labels\n",
    "ax.set_yticks(list(reversed(list(dfFeatImp.index[:30]))))\n",
    "ax.set_yticklabels(dfFeatImp['feature'].head(30))\n",
    "    \n",
    "# Plot labeling\n",
    "plt.xlabel('Normalized Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([i for i in range(1, len(ic_list) + 1)], ic_list)\n",
    "plt.title('IC values on testing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar([i for i in range(1, len(r2_list) + 1)], r2_list)\n",
    "plt.title('R2 values on testing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
