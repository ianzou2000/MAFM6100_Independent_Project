{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.rename(columns={'Unnamed: 0': 'tick'})\n",
    "    df['lastPx'] = df['lastPx'].fillna(method='ffill')\n",
    "    # df['BP1'] = df['BP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    # df['SP1'] = df['SP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    df = df[df['BP1']!=0]\n",
    "    df = df[df['SP1']!=0]\n",
    "    # assert df['avg_price'].isna().sum() == 0\n",
    "    df['mid'] = (df['BP1'] + df['SP1']) / 2\n",
    "    df['return'] = (df['mid'] / df['mid'].shift(1) - 1).fillna(0)\n",
    "    df['diff_v'] = (df['volume'] - df['volume'].shift(1)).fillna(0)\n",
    "    df = df.rename(columns={'lastPx': 'price', 'BP1':'b1', 'BP2':'b2', 'BP3':'b3', 'BP4':'b4', 'BP5':'b5', \n",
    "                            'SP1':'a1', 'SP2':'a2', 'SP3':'a3', 'SP4':'a4', 'SP5':'a5',\n",
    "                            'BV1':'b1_v', 'BV2':'b2_v', 'BV3':'b3_v', 'BV4':'b4_v', 'BV5':'b5_v',\n",
    "                            'SV1':'a1_v', 'SV2':'a2_v', 'SV3':'a3_v', 'SV4':'a4_v', 'SV5':'a5_v',\n",
    "                            'volume': 'volume_sum', 'diff_v': 'volume'})\n",
    "    df['amount'] = df['price'] * df['volume']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF factors\n",
    "def get_realvar(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realskew(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).skew().fillna(0)\n",
    "\n",
    "def get_realkurtosis(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).kurt().fillna(0)\n",
    "\n",
    "def get_realupvar(df, lookback_len, lookback_shift):\n",
    "    df['return_up'] = df['return'][df['return'] > 0]\n",
    "    df['return_up'] = df['return_up'].fillna(0)\n",
    "    return df['return_up'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realdownvar(df, lookback_len, lookback_shift):\n",
    "    df['return_down'] = df['return'][df['return'] < 0]\n",
    "    df['return_down'] = df['return_down'].fillna(0)\n",
    "    return df['return_down'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_ratio_upvar(df, lookback_len, lookback_shift):\n",
    "    return get_realupvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_ratio_downvar(df, lookback_len, lookback_shift):\n",
    "    return get_realdownvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_trendratio(df, lookback_len, lookback_shift):\n",
    "    abs_price_diff = abs(df['price'].diff()).fillna(0)\n",
    "    abs_price_diff_sum = abs_price_diff.shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "    trend_ratio = (df['price']-df['price'].shift(lookback_len)).shift(lookback_shift) / abs_price_diff_sum\n",
    "    return trend_ratio.replace(np.inf, 0).fillna(0)\n",
    "\n",
    "def get_windowreturn(df, lookback_len, lookback_shift):\n",
    "    return np.exp((np.log(df['return']+1)).shift(lookback_shift).rolling(lookback_len).sum())-1\n",
    "\n",
    "def get_minreturn(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).min().fillna(0)\n",
    "\n",
    "def calculate_mdd(series):\n",
    "    max_price = np.maximum.accumulate(series)\n",
    "    drawdown = (max_price - series) / max_price\n",
    "    return np.max(drawdown)\n",
    "\n",
    "def get_mdd(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).apply(lambda x: calculate_mdd(x), raw=True)\n",
    "\n",
    "def get_corrVP_price(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVP_mid(df, lookback_len, lookback_shift):\n",
    "    return df['mid'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVR(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_Amihud(df, lookback_len, lookback_shift):\n",
    "    abs_return = abs(df['return'].diff()).fillna(0)\n",
    "    sum_abs_return = abs_return.shift(lookback_shift).rolling(lookback_len).sum()\n",
    "    return (1 / (lookback_len) * sum_abs_return / df['amount'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_BAspread(df, lookback_len, lookback_shift):\n",
    "    bidsum = df[\"b1\"]*df[\"b1_v\"]+0.8*df[\"b2\"]*df[\"b2_v\"]+0.6*df[\"b3\"]*df[\"b3_v\"]+0.4*df[\"b4\"]*df[\"b4_v\"]+0.2*df[\"b5\"]*df[\"b5_v\"]\n",
    "    asksum = df[\"a1\"]*df[\"a1_v\"]+0.8*df[\"a2\"]*df[\"a2_v\"]+0.6*df[\"a3\"]*df[\"a3_v\"]+0.4*df[\"a4\"]*df[\"a4_v\"]+0.2*df[\"a5\"]*df[\"a5_v\"]\n",
    "    df[\"spread\"] = (bidsum - asksum) / (bidsum + asksum)\n",
    "    return df[\"spread\"].shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def delta_V_A(a1, a1_v):\n",
    "    # a1 and a1_v are ndarrays\n",
    "    diff = a1[-1] - a1[0]\n",
    "    if diff < 0:\n",
    "        return a1_v[-1]\n",
    "    elif diff == 0:\n",
    "        return a1_v[-1] - a1_v[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def delta_V_B(b1, b1_v):\n",
    "    # b1 and b1_v are ndarrays\n",
    "    diff = b1[-1] - b1[0]\n",
    "    if diff < 0:\n",
    "        return 0\n",
    "    elif diff == 0:\n",
    "        return b1_v[-1] - b1_v[0]\n",
    "    else:\n",
    "        return b1_v[-1]\n",
    "    \n",
    "def get_VOI(df, lookback_len, lookback_shift):\n",
    "    delta_Va = np.zeros_like(df['a1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['a1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['a1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Va'] = delta_Va\n",
    "\n",
    "    delta_Vb = np.zeros_like(df['b1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['b1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['b1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Vb'] = delta_Vb\n",
    "\n",
    "    df['ori_VOI'] = df['delta_Vb'] - df['delta_Va']\n",
    "    df['ori_VOI'] = df['ori_VOI'].fillna(0)\n",
    "    df = df.drop(columns=['delta_Va'])\n",
    "    df = df.drop(columns=['delta_Vb'])\n",
    "\n",
    "    mean = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).mean()\n",
    "    std = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).std()\n",
    "    return ((df['ori_VOI'] - mean)/std).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def get_BAspread_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2'] - df['a2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3'] - df['a3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4'] - df['a4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5'] - df['a5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1'] + df['a2'] + df['a3'] + df['a4'] + df['a5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1'] + df['b2'] + df['b3'] + df['b4'] + df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_av_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1_v'] + df['a2_v'] + df['a3_v'] + df['a4_v'] + df['a5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bv_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1_v'] + df['b2_v'] + df['b3_v'] + df['b4_v'] + df['b5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a1_v'] - df['a1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2_v'] - df['a2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3_v'] - df['a3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4_v'] - df['a4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5_v'] - df['a5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1_v'] - df['b1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2_v'] - df['b2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3_v'] - df['b3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4_v'] - df['b4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5_v'] - df['b5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_depth_price_range(df, lookback_len, lookback_shift):\n",
    "    return (df['a1'].shift(lookback_shift).rolling(lookback_len).max() / df['a1'].shift(lookback_shift).rolling(lookback_len).min() - 1).fillna(0)\n",
    "\n",
    "import numba as nb\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def age(prices):\n",
    "    last_value = prices[-1]\n",
    "    age = 0\n",
    "    for i in range(2, len(prices)):\n",
    "        if prices[-i] != last_value:\n",
    "            return age\n",
    "        age += 1\n",
    "    return age\n",
    "\n",
    "def get_BAage(df, lookback_len, lookback_shift):\n",
    "    return df['b1'].shift(lookback_shift).rolling(lookback_len).apply(age, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_cofi(df, lookback_len, lookback_shift):\n",
    "    a = df['b1_v']*np.where(df['b1'].diff()>=0, 1, 0)\n",
    "    b = df['b1_v'].shift()*np.where(df['b1'].diff()<=0, 1, 0)\n",
    "    c = df['a1_v']*np.where(df['a1'].diff()>=0, 1, 0)\n",
    "    d = df['a1_v'].shift()*np.where(df['a1'].diff()<=0, 1, 0)\n",
    "    return (a-b-c+d).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_bp_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['b1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "def get_ap_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['a1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def first_location_of_maximum(x):\n",
    "    max_value = max(x)\n",
    "    for loc in range(len(x)):\n",
    "        if x[loc] == max_value:\n",
    "            return loc + 1\n",
    "        \n",
    "def get_price_idxmax(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(first_location_of_maximum, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def mean_second_derivative_centra(x):\n",
    "    sum_value = 0\n",
    "    for i in range(len(x)-5):\n",
    "        sum_value += (x[i+5]-2*x[i+3]+x[i])/2\n",
    "    return sum_value/(2*(len(x)-5))\n",
    "\n",
    "def get_center_deri_two(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(mean_second_derivative_centra, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_quasi(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].diff(1).abs().shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "\n",
    "def get_weighted_price_to_mid(df, lookback_len, lookback_shift):\n",
    "    avs = df[['a1_v', 'a2_v', 'a3_v', 'a4_v', 'a5_v']].values\n",
    "    bvs = df[['b1_v', 'b2_v', 'b3_v', 'b4_v', 'b5_v']].values\n",
    "    aps = df[['a1', 'a2', 'a3', 'a4', 'a5']].values\n",
    "    bps = df[['b1', 'b2', 'b3', 'b4', 'b5']].values\n",
    "    return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of factors: 414\n"
     ]
    }
   ],
   "source": [
    "lookback_len = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "lookback_shift = 0\n",
    "\n",
    "for i, length in enumerate(lookback_len):\n",
    "    if i == 0:\n",
    "        functions = {f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')}\n",
    "    else:\n",
    "        functions.update({f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')})\n",
    "        \n",
    "\n",
    "print('numbers of factors:', len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "stock_list = ['2330', '2603', '0050']\n",
    "year_list = ['2020', '2021', '2022', '2023']\n",
    "month_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "for stock in stock_list:\n",
    "    df_stock = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            file_name = f'{stock}_md_{year}{month}_{year}{month}.csv'\n",
    "            data_address = f'/Users/ianzou/Desktop/Cornorstone/MAFM_6100/project_code/{stock}/'\n",
    "            try:\n",
    "                df = pd.read_csv(f'{data_address}{file_name}')\n",
    "                print(f'I found {stock} in {year}.{month}. Now you can open it.')\n",
    "            except:\n",
    "                print(f'{stock}: data in {year}.{month} does not exist.')\n",
    "                continue\n",
    "                \n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_prc = pd.DataFrame()\n",
    "\n",
    "            for i, date in enumerate(date_list):\n",
    "                df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "\n",
    "            df = df_prc.copy()\n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_resampled = pd.DataFrame()\n",
    "            for date in date_list:\n",
    "                start_time = pd.to_datetime(f'{date} 09:00:00')\n",
    "                df_tmp = df[df['date'] == date]\n",
    "                df_tmp.loc[:, 'time'] = pd.to_timedelta(df_tmp['time']-90000000, unit='ms') + start_time\n",
    "                # 合并date列和time列成新的datetime列，并设置为索引\n",
    "                df_tmp.set_index(df_tmp['time'], inplace=True)\n",
    "                df_tmp.drop(['time', 'date'], axis=1, inplace=True)\n",
    "                # 每隔1s进行一次下采样\n",
    "                df_tmp = df_tmp.resample('1S').mean()\n",
    "                df_tmp = df_tmp.rename_axis('index')\n",
    "                df_tmp['date'] = df_tmp.index.date.astype(str)\n",
    "                df_tmp['time'] = df_tmp.index.time.astype(str)\n",
    "                df_tmp.reset_index(inplace=True)\n",
    "                df_tmp.drop(columns='index', inplace=True)\n",
    "                df_tmp['label'] = (df_tmp['mid'].shift(-30) / df_tmp['mid'] - 1).fillna(0)\n",
    "\n",
    "                df_resampled = pd.concat([df_resampled, df_tmp])\n",
    "                df_resampled = df_resampled.fillna(0).reset_index(drop=True)\n",
    "\n",
    "            for name, (func, *args) in tqdm(functions.items()):\n",
    "                if 'get_ipython' in name:\n",
    "                    continue\n",
    "                # print(name)\n",
    "                result = func(df_resampled, *args)\n",
    "                var_name = name.replace('get_', '')\n",
    "                df_resampled = pd.concat([df_resampled, result.rename(var_name)], axis=1)\n",
    "\n",
    "            # 定义滚动训练的窗口大小\n",
    "            window_size = 5\n",
    "            date_list = df_resampled['date'].unique().tolist()\n",
    "\n",
    "            # 定义 LASSO 模型的参数\n",
    "            alpha = 0.0003\n",
    "            tol = 1e-4\n",
    "            max_iter = 1000\n",
    "\n",
    "            # 定义 LASSO 模型\n",
    "            # model = Lasso(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "            model = Ridge(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "            # model = LinearRegression()\n",
    "\n",
    "            # 定义用于存储每个 tick 预测结果的列表\n",
    "            r2_list = []\n",
    "            ic_list = []\n",
    "\n",
    "            # 定义用于存储特征重要性的字典\n",
    "            feature_importances = {}\n",
    "\n",
    "            # 对于每个滚动窗口\n",
    "            for i in tqdm(range(window_size, len(date_list))):\n",
    "                # 选择训练数据和目标值\n",
    "                X_train = df_resampled.loc[(df_resampled['date'] < date_list[i]) & (df_resampled['date'] >= date_list[i-window_size]), :]\n",
    "                X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_train = X_train['label']\n",
    "                X_train = X_train.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                # 选择测试数据\n",
    "                X_test = df_resampled.loc[(df_resampled['date'] == date_list[i]), :]\n",
    "                X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_test = X_test['label']\n",
    "                X_test = X_test.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                # 标准化训练数据和测试数据\n",
    "                feature_cols = X_train.columns\n",
    "                # scaler = StandardScaler()\n",
    "                # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "                # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "                # 训练模型\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # 进行预测\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "                r2_list.append(r2)\n",
    "                ic_list.append(corr)\n",
    "\n",
    "                # 记录特征重要性\n",
    "                for feature, coef in zip(feature_cols, model.coef_):\n",
    "                    if feature not in feature_importances:\n",
    "                        feature_importances[feature] = []\n",
    "                    feature_importances[feature].append(coef)\n",
    "                \n",
    "                featureList = []\n",
    "                featureImportance = []\n",
    "\n",
    "                for key in feature_importances:\n",
    "                    featureList.append(key)\n",
    "                    featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "                dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "                dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "                df_stock[f'{date_list[i]}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2, corr]\n",
    "                df_stock.transpose().to_csv(f'./{stock}_ridge_resampled_tmp.csv')\n",
    "\n",
    "                print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "            r2_mean = np.mean(r2_list)\n",
    "            ic_mean = np.mean(ic_list)\n",
    "\n",
    "            # 输出结果\n",
    "            print('平均 R2 值:', r2_mean)\n",
    "            print('平均 IC 值:', ic_mean)\n",
    "\n",
    "            # featureList = []\n",
    "            # featureImportance = []\n",
    "\n",
    "            # for key in feature_importances:\n",
    "            #     featureList.append(key)\n",
    "            #     featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "            # ## Observe the factors' stability\n",
    "            # # Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "            # dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "            # dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "            # df_stock[f'{year}_{month}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2_mean, ic_mean]\n",
    "            # df_stock.transpose().to_csv(f'./{stock}_ridge_resampled_tmp.csv')\n",
    "                \n",
    "    df_stock.transpose().to_csv(f'./{stock}_ridge_resampled.csv')\n",
    "\n",
    "    joblib.dump(model, f'loan_{stock}_ridge.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 0050 in 2023.08. Now you can open it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [07:02<00:00,  1.02s/it]\n",
      "  6%|▋         | 1/16 [00:09<02:23,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 1, IC: 0.1602463574692159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:19<02:15,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 2, IC: 0.15824996361782792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:28<02:04,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 3, IC: 0.2125223218261184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:38<01:56,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 4, IC: 0.13999083486444347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:48<01:47,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 5, IC: 0.20874524933609187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [00:59<01:40, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 6, IC: 0.2322995583369216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [01:08<01:28,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 7, IC: 0.25516446811931265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [01:18<01:19,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 8, IC: 0.23875061327327607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [01:28<01:09,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 9, IC: 0.19596630894206174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [01:39<01:00, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 10, IC: 0.18368168367069398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [01:48<00:50, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 11, IC: 0.18818962025829325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [01:59<00:40, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 12, IC: 0.24286912486603052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [02:09<00:30, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 13, IC: 0.01669208105558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [02:18<00:19,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 14, IC: 0.2013326055563256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [02:27<00:09,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 15, IC: 0.11897511976313302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:37<00:00,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 16, IC: 0.1584167222213578\n",
      "平均 R2 值: 0.003212343092451747\n",
      "平均 IC 值: 0.18200578957354274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 2603 in 2023.08. Now you can open it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [07:01<00:00,  1.02s/it]\n",
      "  6%|▋         | 1/16 [00:09<02:19,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 1, IC: 0.056747586432844284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:20<02:27, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 2, IC: 0.11706363659252352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:31<02:16, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 3, IC: 0.11292682427216894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:41<02:03, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 4, IC: 0.16059637693708437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:50<01:51, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 5, IC: 0.08962496992744609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [01:00<01:38,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 6, IC: 0.03378163665038875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [01:09<01:27,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 7, IC: 0.16422528482245966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [01:19<01:16,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 8, IC: 0.04636031470160251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [01:28<01:06,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 9, IC: 0.12746829083531383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [01:38<00:57,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 10, IC: 0.09100464469971141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [01:47<00:47,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 11, IC: 0.08312392468791416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [01:57<00:38,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 12, IC: 0.03554744136322327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [02:07<00:29,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 13, IC: 0.013002846447693997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [02:18<00:19,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 14, IC: 0.029095207555360304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [02:27<00:09,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 15, IC: 0.008243190753835087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:37<00:00,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training number: 16, completed: 16, IC: 0.032333419870192545\n",
      "平均 R2 值: -0.14419048892903194\n",
      "平均 IC 值: 0.07507159978436016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### predict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "stock_list = ['0050', '2603'] # , '2330'\n",
    "year_list = ['2023']\n",
    "# month_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "month_list = ['08']\n",
    "for stock in stock_list:\n",
    "    df_stock = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            file_name = f'{stock}_md_{year}{month}_{year}{month}.csv'\n",
    "            data_address = f'autodl-tmp/chou/'\n",
    "            try:\n",
    "                df = pd.read_csv(f'{data_address}{file_name}')\n",
    "                print(f'I found {stock} in {year}.{month}. Now you can open it.')\n",
    "            except:\n",
    "                print(f'{stock}: data in {year}.{month} does not exist.')\n",
    "                continue\n",
    "                \n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_prc = pd.DataFrame()\n",
    "\n",
    "            for i, date in enumerate(date_list):\n",
    "                df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "\n",
    "            df = df_prc.copy()\n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_resampled = pd.DataFrame()\n",
    "            for date in date_list:\n",
    "                start_time = pd.to_datetime(f'{date} 09:00:00')\n",
    "                df_tmp = df[df['date'] == date]\n",
    "                df_tmp.loc[:, 'time'] = pd.to_timedelta(df_tmp['time']-90000000, unit='ms') + start_time\n",
    "                # 合并date列和time列成新的datetime列，并设置为索引\n",
    "                df_tmp.set_index(df_tmp['time'], inplace=True)\n",
    "                df_tmp.drop(['time', 'date'], axis=1, inplace=True)\n",
    "                # 每隔1s进行一次下采样\n",
    "                df_tmp = df_tmp.resample('1S').mean()\n",
    "                df_tmp = df_tmp.rename_axis('index')\n",
    "                df_tmp['date'] = df_tmp.index.date.astype(str)\n",
    "                df_tmp['time'] = df_tmp.index.time.astype(str)\n",
    "                df_tmp.reset_index(inplace=True)\n",
    "                df_tmp.drop(columns='index', inplace=True)\n",
    "                df_tmp['label'] = (df_tmp['mid'].shift(-30) / df_tmp['mid'] - 1).fillna(0)\n",
    "\n",
    "                df_resampled = pd.concat([df_resampled, df_tmp])\n",
    "                df_resampled = df_resampled.fillna(0).reset_index(drop=True)\n",
    "\n",
    "            for name, (func, *args) in tqdm(functions.items()):\n",
    "                if 'get_ipython' in name:\n",
    "                    continue\n",
    "                # print(name)\n",
    "                result = func(df_resampled, *args)\n",
    "                var_name = name.replace('get_', '')\n",
    "                df_resampled = pd.concat([df_resampled, result.rename(var_name)], axis=1)\n",
    "\n",
    "            window_size = 5\n",
    "            date_list = df_resampled['date'].unique().tolist()\n",
    "\n",
    "            alpha = 0.0003\n",
    "            tol = 1e-4\n",
    "            max_iter = 1000\n",
    "\n",
    "            model = Ridge(alpha=alpha, tol=tol, max_iter=max_iter)\n",
    "\n",
    "            r2_list = []\n",
    "            ic_list = []\n",
    "\n",
    "            feature_importances = {}\n",
    "\n",
    "            for i in tqdm(range(window_size, len(date_list))):\n",
    "                X_train = df_resampled.loc[(df_resampled['date'] < date_list[i]) & (df_resampled['date'] >= date_list[i-window_size]), :]\n",
    "                X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_train = X_train['label']\n",
    "                X_train = X_train.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                X_test = df_resampled.loc[(df_resampled['date'] == date_list[i]), :]\n",
    "                X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_test = X_test['label']\n",
    "                X_test = X_test.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                feature_cols = X_train.columns\n",
    "                # scaler = StandardScaler()\n",
    "                # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "                # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "                r2_list.append(r2)\n",
    "                ic_list.append(corr)\n",
    "\n",
    "                for feature, coef in zip(feature_cols, model.coef_):\n",
    "                    if feature not in feature_importances:\n",
    "                        feature_importances[feature] = []\n",
    "                    feature_importances[feature].append(coef)\n",
    "                \n",
    "                featureList = []\n",
    "                featureImportance = []\n",
    "\n",
    "                for key in feature_importances:\n",
    "                    featureList.append(key)\n",
    "                    featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "                # Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "                dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "                dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "                df_stock[f'{date_list[i]}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2, corr]\n",
    "                # df_stock.transpose().to_csv(f'./{stock}_lasso_resampled_tmp.csv')\n",
    "\n",
    "                print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "            r2_mean = np.mean(r2_list)\n",
    "            ic_mean = np.mean(ic_list)\n",
    "\n",
    "            # print('特征重要性:', feature_importances)\n",
    "            print('平均 R2 值:', r2_mean)\n",
    "            print('平均 IC 值:', ic_mean)\n",
    "\n",
    "            # featureList = []\n",
    "            # featureImportance = []\n",
    "\n",
    "            # for key in feature_importances:\n",
    "            #     featureList.append(key)\n",
    "            #     featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "            # # Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "            # dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "            # dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "            # df_stock[f'{year}_{month}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2_mean, ic_mean]\n",
    "            # df_stock.transpose().to_csv(f'./{stock}_ridge_resampled_tmp.csv')\n",
    "    import joblib\n",
    "    joblib.dump(model, f'loan_{stock}_lasso.pkl')\n",
    "    \n",
    "    # df_stock.transpose().to_csv(f'./{stock}_ridge_resampled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
