{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.rename(columns={'Unnamed: 0': 'tick'})\n",
    "    df['lastPx'] = df['lastPx'].fillna(method='ffill')\n",
    "    df['BP1'] = df['BP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    df['SP1'] = df['SP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    # assert df['avg_price'].isna().sum() == 0\n",
    "    df['mid'] = (df['BP1'] + df['SP1']) / 2\n",
    "    df['return'] = (df['mid'] / df['mid'].shift(1) - 1).fillna(0)\n",
    "    df['diff_v'] = (df['volume'] - df['volume'].shift(1)).fillna(0)\n",
    "    df = df.rename(columns={'lastPx': 'price', 'BP1':'b1', 'BP2':'b2', 'BP3':'b3', 'BP4':'b4', 'BP5':'b5', \n",
    "                            'SP1':'a1', 'SP2':'a2', 'SP3':'a3', 'SP4':'a4', 'SP5':'a5',\n",
    "                            'BV1':'b1_v', 'BV2':'b2_v', 'BV3':'b3_v', 'BV4':'b4_v', 'BV5':'b5_v',\n",
    "                            'SV1':'a1_v', 'SV2':'a2_v', 'SV3':'a3_v', 'SV4':'a4_v', 'SV5':'a5_v',\n",
    "                            'volume': 'volume_sum', 'diff_v': 'volume'})\n",
    "    df['amount'] = df['price'] * df['volume']\n",
    "    df = df[df['b1']!=0]\n",
    "    df = df[df['a1']!=0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF factors\n",
    "def get_realvar(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realskew(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).skew().fillna(0)\n",
    "\n",
    "def get_realkurtosis(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).kurt().fillna(0)\n",
    "\n",
    "def get_realupvar(df, lookback_len, lookback_shift):\n",
    "    df['return_up'] = df['return'][df['return'] > 0]\n",
    "    df['return_up'] = df['return_up'].fillna(0)\n",
    "    return df['return_up'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realdownvar(df, lookback_len, lookback_shift):\n",
    "    df['return_down'] = df['return'][df['return'] < 0]\n",
    "    df['return_down'] = df['return_down'].fillna(0)\n",
    "    return df['return_down'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_ratio_upvar(df, lookback_len, lookback_shift):\n",
    "    return get_realupvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_ratio_downvar(df, lookback_len, lookback_shift):\n",
    "    return get_realdownvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_trendratio(df, lookback_len, lookback_shift):\n",
    "    abs_price_diff = abs(df['price'].diff()).fillna(0)\n",
    "    abs_price_diff_sum = abs_price_diff.shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "    trend_ratio = (df['price']-df['price'].shift(lookback_len)).shift(lookback_shift) / abs_price_diff_sum\n",
    "    return trend_ratio.replace(np.inf, 0).fillna(0)\n",
    "\n",
    "def get_windowreturn(df, lookback_len, lookback_shift):\n",
    "    return np.exp((np.log(df['return']+1)).shift(lookback_shift).rolling(lookback_len).sum())-1\n",
    "\n",
    "def get_minreturn(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).min().fillna(0)\n",
    "\n",
    "def calculate_mdd(series):\n",
    "    max_price = np.maximum.accumulate(series)\n",
    "    drawdown = (max_price - series) / max_price\n",
    "    return np.max(drawdown)\n",
    "\n",
    "def get_mdd(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).apply(lambda x: calculate_mdd(x), raw=True)\n",
    "\n",
    "def get_corrVP_price(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVP_mid(df, lookback_len, lookback_shift):\n",
    "    return df['mid'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVR(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_Amihud(df, lookback_len, lookback_shift):\n",
    "    abs_return = abs(df['return'].diff()).fillna(0)\n",
    "    sum_abs_return = abs_return.shift(lookback_shift).rolling(lookback_len).sum()\n",
    "    return (1 / (lookback_len) * sum_abs_return / df['amount'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_BAspread(df, lookback_len, lookback_shift):\n",
    "    bidsum = df[\"b1\"]*df[\"b1_v\"]+0.8*df[\"b2\"]*df[\"b2_v\"]+0.6*df[\"b3\"]*df[\"b3_v\"]+0.4*df[\"b4\"]*df[\"b4_v\"]+0.2*df[\"b5\"]*df[\"b5_v\"]\n",
    "    asksum = df[\"a1\"]*df[\"a1_v\"]+0.8*df[\"a2\"]*df[\"a2_v\"]+0.6*df[\"a3\"]*df[\"a3_v\"]+0.4*df[\"a4\"]*df[\"a4_v\"]+0.2*df[\"a5\"]*df[\"a5_v\"]\n",
    "    df[\"spread\"] = (bidsum - asksum) / (bidsum + asksum)\n",
    "    return df[\"spread\"].shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def delta_V_A(a1, a1_v):\n",
    "    # a1 and a1_v are ndarrays\n",
    "    diff = a1[-1] - a1[0]\n",
    "    if diff < 0:\n",
    "        return a1_v[-1]\n",
    "    elif diff == 0:\n",
    "        return a1_v[-1] - a1_v[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def delta_V_B(b1, b1_v):\n",
    "    # b1 and b1_v are ndarrays\n",
    "    diff = b1[-1] - b1[0]\n",
    "    if diff < 0:\n",
    "        return 0\n",
    "    elif diff == 0:\n",
    "        return b1_v[-1] - b1_v[0]\n",
    "    else:\n",
    "        return b1_v[-1]\n",
    "    \n",
    "def get_VOI(df, lookback_len, lookback_shift):\n",
    "    delta_Va = np.zeros_like(df['a1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['a1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['a1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Va'] = delta_Va\n",
    "\n",
    "    delta_Vb = np.zeros_like(df['b1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['b1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['b1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Vb'] = delta_Vb\n",
    "\n",
    "    df['ori_VOI'] = df['delta_Vb'] - df['delta_Va']\n",
    "    df['ori_VOI'] = df['ori_VOI'].fillna(0)\n",
    "    df = df.drop(columns=['delta_Va'])\n",
    "    df = df.drop(columns=['delta_Vb'])\n",
    "\n",
    "    mean = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).mean()\n",
    "    std = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).std()\n",
    "    return ((df['ori_VOI'] - mean)/std).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def get_BAspread_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2'] - df['a2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3'] - df['a3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4'] - df['a4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5'] - df['a5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1'] + df['a2'] + df['a3'] + df['a4'] + df['a5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1'] + df['b2'] + df['b3'] + df['b4'] + df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_av_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1_v'] + df['a2_v'] + df['a3_v'] + df['a4_v'] + df['a5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bv_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1_v'] + df['b2_v'] + df['b3_v'] + df['b4_v'] + df['b5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a1_v'] - df['a1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2_v'] - df['a2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3_v'] - df['a3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4_v'] - df['a4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5_v'] - df['a5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1_v'] - df['b1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2_v'] - df['b2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3_v'] - df['b3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4_v'] - df['b4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5_v'] - df['b5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_depth_price_range(df, lookback_len, lookback_shift):\n",
    "    return (df['a1'].shift(lookback_shift).rolling(lookback_len).max() / df['a1'].shift(lookback_shift).rolling(lookback_len).min() - 1).fillna(0)\n",
    "\n",
    "import numba as nb\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def age(prices):\n",
    "    last_value = prices[-1]\n",
    "    age = 0\n",
    "    for i in range(2, len(prices)):\n",
    "        if prices[-i] != last_value:\n",
    "            return age\n",
    "        age += 1\n",
    "    return age\n",
    "\n",
    "def get_BAage(df, lookback_len, lookback_shift):\n",
    "    return df['b1'].shift(lookback_shift).rolling(lookback_len).apply(age, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_cofi(df, lookback_len, lookback_shift):\n",
    "    a = df['b1_v']*np.where(df['b1'].diff()>=0, 1, 0)\n",
    "    b = df['b1_v'].shift()*np.where(df['b1'].diff()<=0, 1, 0)\n",
    "    c = df['a1_v']*np.where(df['a1'].diff()>=0, 1, 0)\n",
    "    d = df['a1_v'].shift()*np.where(df['a1'].diff()<=0, 1, 0)\n",
    "    return (a-b-c+d).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_bp_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['b1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "def get_ap_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['a1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def first_location_of_maximum(x):\n",
    "    max_value = max(x)\n",
    "    for loc in range(len(x)):\n",
    "        if x[loc] == max_value:\n",
    "            return loc + 1\n",
    "        \n",
    "def get_price_idxmax(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(first_location_of_maximum, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def mean_second_derivative_centra(x):\n",
    "    sum_value = 0\n",
    "    for i in range(len(x)-5):\n",
    "        sum_value += (x[i+5]-2*x[i+3]+x[i])/2\n",
    "    return sum_value/(2*(len(x)-5))\n",
    "\n",
    "def get_center_deri_two(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(mean_second_derivative_centra, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_quasi(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].diff(1).abs().shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "\n",
    "def get_weighted_price_to_mid(df, lookback_len, lookback_shift):\n",
    "    avs = df[['a1_v', 'a2_v', 'a3_v', 'a4_v', 'a5_v']].values\n",
    "    bvs = df[['b1_v', 'b2_v', 'b3_v', 'b4_v', 'b5_v']].values\n",
    "    aps = df[['a1', 'a2', 'a3', 'a4', 'a5']].values\n",
    "    bps = df[['b1', 'b2', 'b3', 'b4', 'b5']].values\n",
    "    return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of factors: 414\n"
     ]
    }
   ],
   "source": [
    "lookback_len = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "lookback_shift = 0\n",
    "\n",
    "for i, length in enumerate(lookback_len):\n",
    "    if i == 0:\n",
    "        functions = {f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')}\n",
    "    else:\n",
    "        functions.update({f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')})\n",
    "        \n",
    "\n",
    "print('numbers of factors:', len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0050: data in 2020.01 does not exist.\n",
      "0050: data in 2020.02 does not exist.\n",
      "0050: data in 2020.03 does not exist.\n",
      "0050: data in 2020.04 does not exist.\n",
      "I found 0050 in 2020.05. Now you can open it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [04:34<00:00,  1.51it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "### train\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "stock_list = ['0050', '2330', '2603']\n",
    "year_list = ['2020', '2021', '2022', '2023']\n",
    "month_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "for stock in stock_list:\n",
    "    df_stock = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            file_name = f'{stock}_md_{year}{month}_{year}{month}.csv'\n",
    "            data_address = f'/Users/ianzou/Desktop/Cornorstone/MAFM_6100/project_code/{stock}/'\n",
    "            try:\n",
    "                df = pd.read_csv(f'{data_address}{file_name}')\n",
    "                print(f'I found {stock} in {year}.{month}. Now you can open it.')\n",
    "            except:\n",
    "                print(f'{stock}: data in {year}.{month} does not exist.')\n",
    "                continue\n",
    "                \n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_prc = pd.DataFrame()\n",
    "\n",
    "            for i, date in enumerate(date_list):\n",
    "                df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "\n",
    "            df = df_prc.copy()\n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_resampled = pd.DataFrame()\n",
    "            for date in date_list:\n",
    "                start_time = pd.to_datetime(f'{date} 09:00:00')\n",
    "                df_tmp = df[df['date'] == date]\n",
    "                df_tmp.loc[:, 'time'] = pd.to_timedelta(df_tmp['time']-90000000, unit='ms') + start_time\n",
    "                # 合并date列和time列成新的datetime列，并设置为索引\n",
    "                df_tmp.set_index(df_tmp['time'], inplace=True)\n",
    "                df_tmp.drop(['time', 'date'], axis=1, inplace=True)\n",
    "                # 每隔1s进行一次下采样\n",
    "                df_tmp = df_tmp.resample('1S').mean()\n",
    "                df_tmp = df_tmp.rename_axis('index')\n",
    "                df_tmp['date'] = df_tmp.index.date.astype(str)\n",
    "                df_tmp['time'] = df_tmp.index.time.astype(str)\n",
    "                df_tmp.reset_index(inplace=True)\n",
    "                df_tmp.drop(columns='index', inplace=True)\n",
    "                df_tmp['label'] = (df_tmp['mid'].shift(-30) / df_tmp['mid'] - 1).fillna(0)\n",
    "\n",
    "                df_resampled = pd.concat([df_resampled, df_tmp])\n",
    "                df_resampled = df_resampled.fillna(0).reset_index(drop=True)\n",
    "\n",
    "            for name, (func, *args) in tqdm(functions.items()):\n",
    "                if 'get_ipython' in name:\n",
    "                    continue\n",
    "                # print(name)\n",
    "                result = func(df_resampled, *args)\n",
    "                var_name = name.replace('get_', '')\n",
    "                df_resampled = pd.concat([df_resampled, result.rename(var_name)], axis=1)\n",
    "\n",
    "            # 定义滚动训练的窗口大小\n",
    "            window_size = 5\n",
    "            date_list = df_resampled['date'].unique().tolist()\n",
    "\n",
    "            # 定义 LightGBM 模型的参数\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mse',\n",
    "                'learning_rate': 0.001,\n",
    "                'num_leaves': 31,\n",
    "                'max_depth': -1,\n",
    "                'n_estimators': 1000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "\n",
    "            # 定义 LGBM 模型\n",
    "            model = lgb.LGBMRegressor(boosting_type='gbdt', **params)\n",
    "\n",
    "            # 定义用于存储每个 tick 预测结果的列表\n",
    "            r2_list = []\n",
    "            ic_list = []\n",
    "\n",
    "            # 定义用于存储特征重要性的字典\n",
    "            feature_importances = {}\n",
    "\n",
    "            # 对于每个滚动窗口\n",
    "            for i in tqdm(range(window_size, len(date_list))):\n",
    "                # 选择训练数据和目标值\n",
    "                X_train = df_resampled.loc[(df_resampled['date'] < date_list[i]) & (df_resampled['date'] >= date_list[i-window_size]), :]\n",
    "                X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_train = X_train['label']\n",
    "                X_train = X_train.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                # 选择测试数据\n",
    "                X_test = df_resampled.loc[(df_resampled['date'] == date_list[i]), :]\n",
    "                X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "                y_test = X_test['label']\n",
    "                X_test = X_test.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "                # 标准化训练数据和测试数据\n",
    "                feature_cols = X_train.columns\n",
    "                # scaler = StandardScaler()\n",
    "                # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "                # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "                # 训练模型\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # 进行预测\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # 计算预测值和真实值之间的R方\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "                # 将R方添加到列表中\n",
    "                r2_list.append(r2)\n",
    "                ic_list.append(corr)\n",
    "\n",
    "                # 记录特征重要性\n",
    "                for feature, coef in zip(feature_cols, model.coef_):\n",
    "                    if feature not in feature_importances:\n",
    "                        feature_importances[feature] = []\n",
    "                    feature_importances[feature].append(coef)\n",
    "                \n",
    "                featureList = []\n",
    "                featureImportance = []\n",
    "\n",
    "                for key in feature_importances:\n",
    "                    featureList.append(key)\n",
    "                    featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "                ## Draw Feature Importance\n",
    "                # Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "                dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "                dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "                df_stock[f'{date_list[i]}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2, corr]\n",
    "                df_stock.transpose().to_csv(f'./{stock}_lasso_resampled_tmp.csv')\n",
    "\n",
    "                print(f'Total training number: {len(date_list)-window_size}, completed: {i-window_size+1}, IC: {corr}')\n",
    "\n",
    "            # 计算所有天预测的平均 R2 值\n",
    "            r2_mean = np.mean(r2_list)\n",
    "            ic_mean = np.mean(ic_list)\n",
    "\n",
    "            # 输出结果\n",
    "            # print('特征重要性:', feature_importances)\n",
    "            print('平均 R2 值:', r2_mean)\n",
    "            print('平均 IC 值:', ic_mean)\n",
    "\n",
    "            # featureList = []\n",
    "            # featureImportance = []\n",
    "\n",
    "            # for key in feature_importances:\n",
    "            #     featureList.append(key)\n",
    "            #     featureImportance.append(np.mean(feature_importances[key]))\n",
    "\n",
    "            # ## Draw Feature Importance\n",
    "            # # Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "            # dfFeatImp = pd.DataFrame({'feature': featureList, 'importance': featureImportance})\n",
    "            # dfFeatImp = dfFeatImp.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "            # df_stock[f'{year}_{month}'] = list(dfFeatImp['feature'].iloc[:10].reset_index(drop=True))+[r2_mean, ic_mean]\n",
    "            # df_stock.transpose().to_csv(f'./{stock}_lasso_resampled_tmp.csv')\n",
    "                \n",
    "    df_stock.transpose().to_csv(f'./{stock}_lgbm_resampled.csv')\n",
    "    joblib.dump(model, f'loan_{stock}_lgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### predict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "stock_list = ['2603', '2330', '0050']\n",
    "year_list = ['2023']\n",
    "# month_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "month_list = ['09']\n",
    "\n",
    "for stock in stock_list:\n",
    "    result_df = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            file_name = f'{stock}_md_{year}{month}_{year}{month}.csv'\n",
    "            data_address = f'autodl-tmp/chou/'\n",
    "            try:\n",
    "                df = pd.read_csv(f'{data_address}{file_name}')\n",
    "                print(f'I found {stock} in {year}.{month}. Now you can open it.')\n",
    "            except:\n",
    "                print(f'{stock}: data in {year}.{month} does not exist.')\n",
    "                continue\n",
    "                \n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_prc = pd.DataFrame()\n",
    "\n",
    "            for i, date in enumerate(date_list):\n",
    "                df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "\n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_prc = pd.DataFrame()\n",
    "\n",
    "            for i, date in enumerate(date_list):\n",
    "                df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "\n",
    "            df = df_prc.copy()\n",
    "            date_list = df['date'].unique().tolist()\n",
    "            df_resampled = pd.DataFrame()\n",
    "            for date in date_list:\n",
    "                start_time = pd.to_datetime(f'{date} 09:00:00')\n",
    "                df_tmp = df[df['date'] == date]\n",
    "                df_tmp.loc[:, 'time'] = pd.to_timedelta(df_tmp['time']-90000000, unit='ms') + start_time\n",
    "                # 合并date列和time列成新的datetime列，并设置为索引\n",
    "                df_tmp.set_index(df_tmp['time'], inplace=True)\n",
    "                df_tmp.drop(['time', 'date'], axis=1, inplace=True)\n",
    "                # 每隔1s进行一次下采样\n",
    "                df_tmp = df_tmp.resample('1S').mean()\n",
    "                df_tmp = df_tmp.rename_axis('index')\n",
    "                df_tmp['date'] = df_tmp.index.date.astype(str)\n",
    "                df_tmp['time'] = df_tmp.index.time.astype(str)\n",
    "                df_tmp.reset_index(inplace=True)\n",
    "                df_tmp.drop(columns='index', inplace=True)\n",
    "                df_tmp['label'] = (df_tmp['mid'].shift(-30) / df_tmp['mid'] - 1).fillna(0)\n",
    "\n",
    "                df_resampled = pd.concat([df_resampled, df_tmp])\n",
    "                df_resampled = df_resampled.fillna(0).reset_index(drop=True)\n",
    "\n",
    "            for name, (func, *args) in tqdm(functions.items()):\n",
    "                if 'get_ipython' in name:\n",
    "                    continue\n",
    "                result = func(df_resampled, *args)\n",
    "                var_name = name.replace('get_', '')\n",
    "                df_resampled = pd.concat([df_resampled, result.rename(var_name)], axis=1)\n",
    "                \n",
    "            model = joblib.load(f'loan_{stock}_lgbm.pkl')\n",
    "            X_test = df_resampled\n",
    "            X_date = list(X_test['date'])\n",
    "            X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "            y_test = list(X_test['label'])\n",
    "            X_test = X_test.drop(['label'], axis=1).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "            y_pred = list(model.predict(X_test))\n",
    "\n",
    "            # Save the results into the overall DataFrame\n",
    "            result_df = pd.concat([result_df, pd.DataFrame({'date': X_date, 'y_pred': y_pred, 'y_true': y_test})], ignore_index=True)\n",
    "\n",
    "    # After all iterations, save the final DataFrame to a CSV file\n",
    "    result_df.to_csv(f'./predict_{stock}_lgbm_resample.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
