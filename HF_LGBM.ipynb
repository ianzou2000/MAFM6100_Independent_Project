{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e835d53f-61b7-4ee5-ab6d-d59d0de49673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0095f3a3-e244-4a4c-ad9d-8f2421a86612",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF factors\n",
    "def get_realvar(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realskew(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).skew().fillna(0)\n",
    "\n",
    "def get_realkurtosis(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).kurt().fillna(0)\n",
    "\n",
    "def get_realupvar(df, lookback_len, lookback_shift):\n",
    "    df['return_up'] = df['return'][df['return'] > 0]\n",
    "    df['return_up'] = df['return_up'].fillna(0)\n",
    "    return df['return_up'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_realdownvar(df, lookback_len, lookback_shift):\n",
    "    df['return_down'] = df['return'][df['return'] < 0]\n",
    "    df['return_down'] = df['return_down'].fillna(0)\n",
    "    return df['return_down'].shift(lookback_shift).rolling(lookback_len).var().fillna(0)\n",
    "\n",
    "def get_ratio_upvar(df, lookback_len, lookback_shift):\n",
    "    return get_realupvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_ratio_downvar(df, lookback_len, lookback_shift):\n",
    "    return get_realdownvar(df, lookback_len, lookback_shift) / get_realvar(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_trendratio(df, lookback_len, lookback_shift):\n",
    "    abs_price_diff = abs(df['price'].diff()).fillna(0)\n",
    "    abs_price_diff_sum = abs_price_diff.shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "    trend_ratio = (df['price']-df['price'].shift(lookback_len)).shift(lookback_shift) / abs_price_diff_sum\n",
    "    return trend_ratio.replace(np.inf, 0).fillna(0)\n",
    "\n",
    "def get_windowreturn(df, lookback_len, lookback_shift):\n",
    "    return np.exp((np.log(df['return']+1)).shift(lookback_shift).rolling(lookback_len).sum())-1\n",
    "\n",
    "def get_minreturn(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).min().fillna(0)\n",
    "\n",
    "def calculate_mdd(series):\n",
    "    max_price = np.maximum.accumulate(series)\n",
    "    drawdown = (max_price - series) / max_price\n",
    "    return np.max(drawdown)\n",
    "\n",
    "def get_mdd(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).apply(lambda x: calculate_mdd(x), raw=True)\n",
    "\n",
    "def get_corrVP_price(df, lookback_len, lookback_shift):\n",
    "    return df['price'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "# def get_corrVP_avg(df, lookback_len, lookback_shift):\n",
    "#     return df['price'].fillna(method='ffill').shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVP_mid(df, lookback_len, lookback_shift):\n",
    "    return df['mid'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_corrVR(df, lookback_len, lookback_shift):\n",
    "    return df['return'].shift(lookback_shift).rolling(lookback_len).corr(df['volume'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_Amihud(df, lookback_len, lookback_shift):\n",
    "    abs_return = abs(df['return'].diff()).fillna(0)\n",
    "    sum_abs_return = abs_return.shift(lookback_shift).rolling(lookback_len).sum()\n",
    "    return (1 / (lookback_len) * sum_abs_return / df['amount'].shift(lookback_shift)).fillna(0)\n",
    "\n",
    "def get_BAspread(df, lookback_len, lookback_shift):\n",
    "    bidsum = df[\"b1\"]*df[\"b1_v\"]+0.8*df[\"b2\"]*df[\"b2_v\"]+0.6*df[\"b3\"]*df[\"b3_v\"]+0.4*df[\"b4\"]*df[\"b4_v\"]+0.2*df[\"b5\"]*df[\"b5_v\"]\n",
    "    asksum = df[\"a1\"]*df[\"a1_v\"]+0.8*df[\"a2\"]*df[\"a2_v\"]+0.6*df[\"a3\"]*df[\"a3_v\"]+0.4*df[\"a4\"]*df[\"a4_v\"]+0.2*df[\"a5\"]*df[\"a5_v\"]\n",
    "    df[\"spread\"] = (bidsum - asksum) / (bidsum + asksum)\n",
    "    return df[\"spread\"].shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def delta_V_A(a1, a1_v):\n",
    "    # a1 and a1_v are ndarrays\n",
    "    diff = a1[-1] - a1[0]\n",
    "    if diff < 0:\n",
    "        return a1_v[-1]\n",
    "    elif diff == 0:\n",
    "        return a1_v[-1] - a1_v[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def delta_V_B(b1, b1_v):\n",
    "    # b1 and b1_v are ndarrays\n",
    "    diff = b1[-1] - b1[0]\n",
    "    if diff < 0:\n",
    "        return 0\n",
    "    elif diff == 0:\n",
    "        return b1_v[-1] - b1_v[0]\n",
    "    else:\n",
    "        return b1_v[-1]\n",
    "    \n",
    "def get_VOI(df, lookback_len, lookback_shift):\n",
    "    delta_Va = np.zeros_like(df['a1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['a1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['a1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Va'] = delta_Va\n",
    "\n",
    "    delta_Vb = np.zeros_like(df['b1_v'])\n",
    "    for i in range(1, len(df)):\n",
    "        a1_slice = df['b1'].values[i-1:i+1]\n",
    "        a1_v_slice = df['b1_v'].values[i-1:i+1]\n",
    "        delta_Va[i] = delta_V_A(a1_slice, a1_v_slice)\n",
    "    df['delta_Vb'] = delta_Vb\n",
    "\n",
    "    df['ori_VOI'] = df['delta_Vb'] - df['delta_Va']\n",
    "    df['ori_VOI'] = df['ori_VOI'].fillna(0)\n",
    "    df = df.drop(columns=['delta_Va'])\n",
    "    df = df.drop(columns=['delta_Vb'])\n",
    "\n",
    "    mean = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).mean()\n",
    "    std = df['ori_VOI'].shift(lookback_shift).rolling(lookback_len).std()\n",
    "    return ((df['ori_VOI'] - mean)/std).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def get_BAspread_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2'] - df['a2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3'] - df['a3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4'] - df['a4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_BAspread_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5'] - df['a5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_midprice_1_mean(df, lookback_len, lookback_shift):\n",
    "    return ((df['b1']+df['a1'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_midprice_2_mean(df, lookback_len, lookback_shift):\n",
    "    return ((df['b2']+df['a2'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_midprice_3_mean(df, lookback_len, lookback_shift):\n",
    "    return ((df['b3']+df['a3'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_midprice_4_mean(df, lookback_len, lookback_shift):\n",
    "    return ((df['b4']+df['a4'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_midprice_5_mean(df, lookback_len, lookback_shift):\n",
    "    return ((df['b5']+df['a5'])/2).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5'] - df['a1']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['b2']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['b3']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['b4']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['b5']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_ap_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['a2'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_ap_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['a3'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_ap_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['a4'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_ap_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['a5'] - df['a1'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_bp_diff_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['b1'] - df['b2'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_bp_diff_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['b1'] - df['b3'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_bp_diff_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['b1'] - df['b4'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_abs_bp_diff_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (abs(df['b1'] - df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_ap_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1'] + df['a2'] + df['a3'] + df['a4'] + df['a5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bp_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1'] + df['b2'] + df['b3'] + df['b4'] + df['b5'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_av_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['a1_v'] + df['a2_v'] + df['a3_v'] + df['a4_v'] + df['a5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_bv_sum_mean(df, lookback_len, lookback_shift):\n",
    "    return (1/5 * (df['b1_v'] + df['b2_v'] + df['b3_v'] + df['b4_v'] + df['b5_v'])).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_sum_ap_diff_mean(df, lookback_len, lookback_shift):\n",
    "    return get_ap_diff_1_mean(df, lookback_len, lookback_shift) + get_ap_diff_2_mean(df, lookback_len, lookback_shift) + get_ap_diff_3_mean(df, lookback_len, lookback_shift) + get_ap_diff_4_mean(df, lookback_len, lookback_shift)\n",
    "def get_sum_bp_diff_mean(df, lookback_len, lookback_shift):\n",
    "    return get_bp_diff_1_mean(df, lookback_len, lookback_shift) + get_bp_diff_2_mean(df, lookback_len, lookback_shift) + get_bp_diff_3_mean(df, lookback_len, lookback_shift) + get_bp_diff_4_mean(df, lookback_len, lookback_shift)\n",
    "\n",
    "def get_deriv_ap_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a1'] - df['a1'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_ap_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2'] - df['a2'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_ap_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3'] - df['a3'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_ap_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4'] - df['a4'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_ap_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5'] - df['a5'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bp_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1'] - df['b1'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bp_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2'] - df['b2'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bp_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3'] - df['b3'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bp_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4'] - df['b4'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bp_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5'] - df['b5'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a1_v'] - df['a1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a2_v'] - df['a2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a3_v'] - df['a3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a4_v'] - df['a4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_av_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['a5_v'] - df['a5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_1_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b1_v'] - df['b1_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_2_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b2_v'] - df['b2_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_3_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b3_v'] - df['b3_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_4_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b4_v'] - df['b4_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "def get_deriv_bv_5_mean(df, lookback_len, lookback_shift):\n",
    "    return (df['b5_v'] - df['b5_v'].shift(2)).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_depth_price_range(df, lookback_len, lookback_shift):\n",
    "    return (df['a1'].shift(lookback_shift).rolling(lookback_len).max() / df['a1'].shift(lookback_shift).rolling(lookback_len).min() - 1).fillna(0)\n",
    "\n",
    "import numba as nb\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def age(prices):\n",
    "    last_value = prices[-1]\n",
    "    age = 0\n",
    "    for i in range(2, len(prices)):\n",
    "        if prices[-i] != last_value:\n",
    "            return age\n",
    "        age += 1\n",
    "    return age\n",
    "\n",
    "def get_BAage(df, lookback_len, lookback_shift):\n",
    "    return df['b1'].shift(lookback_shift).rolling(lookback_len).apply(age, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_cofi(df, lookback_len, lookback_shift):\n",
    "    a = df['b1_v']*np.where(df['b1'].diff()>=0, 1, 0)\n",
    "    b = df['b1_v'].shift()*np.where(df['b1'].diff()<=0, 1, 0)\n",
    "    c = df['a1_v']*np.where(df['a1'].diff()>=0, 1, 0)\n",
    "    d = df['a1_v'].shift()*np.where(df['a1'].diff()<=0, 1, 0)\n",
    "    return (a-b-c+d).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
    "\n",
    "def get_bp_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['b1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "def get_ap_rank(df, lookback_len, lookback_shift):\n",
    "    return ((df['a1'].shift(lookback_shift).rolling(lookback_len).rank()) / lookback_len*2 - 1).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def first_location_of_maximum(x):\n",
    "    max_value = max(x)\n",
    "    for loc in range(len(x)):\n",
    "        if x[loc] == max_value:\n",
    "            return loc + 1\n",
    "        \n",
    "def get_price_idxmax(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(first_location_of_maximum, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def mean_second_derivative_centra(x):\n",
    "    sum_value = 0\n",
    "    for i in range(len(x)-5):\n",
    "        sum_value += (x[i+5]-2*x[i+3]+x[i])/2\n",
    "    return sum_value/(2*(len(x)-5))\n",
    "\n",
    "def get_center_deri_two(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].shift(lookback_shift).rolling(lookback_len).apply(mean_second_derivative_centra, engine='numba', raw=True).fillna(0)\n",
    "\n",
    "def get_quasi(df, lookback_len, lookback_shift):\n",
    "    return df['a1'].diff(1).abs().shift(lookback_shift).rolling(lookback_len).sum().fillna(0)\n",
    "\n",
    "def get_weighted_price_to_mid(df, lookback_len, lookback_shift):\n",
    "    avs = df[['a1_v', 'a2_v', 'a3_v', 'a4_v', 'a5_v']].values\n",
    "    bvs = df[['b1_v', 'b2_v', 'b3_v', 'b4_v', 'b5_v']].values\n",
    "    aps = df[['a1', 'a2', 'a3', 'a4', 'a5']].values\n",
    "    bps = df[['b1', 'b2', 'b3', 'b4', 'b5']].values\n",
    "    return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d745f886-c03f-42d6-9be1-7f4b95be37de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of factors: 711\n"
     ]
    }
   ],
   "source": [
    "lookback_len = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "lookback_shift = 0\n",
    "\n",
    "for i, length in enumerate(lookback_len):\n",
    "    if i == 0:\n",
    "        functions = {f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')}\n",
    "    else:\n",
    "        functions.update({f'{k}_{lookback_shift}_{length}': (v, length, lookback_shift) for k, v in globals().items() if callable(v) and k.startswith('get_')})\n",
    "        \n",
    "\n",
    "print('numbers of factors:', len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9d3973-df5d-4d70-b816-86a0a5e68599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.rename(columns={'Unnamed: 0': 'tick'})\n",
    "    df['lastPx'] = df['lastPx'].fillna(method='ffill')\n",
    "    df['BP1'] = df['BP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    df['SP1'] = df['SP1'].replace(0, np.nan).fillna(method='ffill')\n",
    "    # assert df['avg_price'].isna().sum() == 0\n",
    "    df = df[df['BP1'] != 0]\n",
    "    df = df[df['SP1'] != 0]\n",
    "    df['mid'] = (df['BP1'] + df['SP1']) / 2\n",
    "    df['diff_v'] = (df['volume'] - df['volume'].shift(1)).fillna(0)\n",
    "    df['return'] = (df['mid'] / df['mid'].shift(1) - 1).fillna(0)\n",
    "    df = df.rename(columns={'lastPx': 'price', 'BP1':'b1', 'BP2':'b2', 'BP3':'b3', 'BP4':'b4', 'BP5':'b5', \n",
    "                            'SP1':'a1', 'SP2':'a2', 'SP3':'a3', 'SP4':'a4', 'SP5':'a5',\n",
    "                            'BV1':'b1_v', 'BV2':'b2_v', 'BV3':'b3_v', 'BV4':'b4_v', 'BV5':'b5_v',\n",
    "                            'SV1':'a1_v', 'SV2':'a2_v', 'SV3':'a3_v', 'SV4':'a4_v', 'SV5':'a5_v',\n",
    "                            'volume': 'volume_sum', 'diff_v': 'volume'})\n",
    "    df['amount'] = df['price'] * df['volume']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75894e4-4279-4212-b5ca-bf2de470c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./2330/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '2330':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f956670b-e6e4-4bfb-8f07-bb899afd2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9780377d-40fa-42e8-b17e-795697cd797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73881\n",
      "[LightGBM] [Info] Number of data points in the train set: 123828, number of used features: 665\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 73921\n",
      "[LightGBM] [Info] Number of data points in the train set: 125543, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72091\n",
      "[LightGBM] [Info] Number of data points in the train set: 120321, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72597\n",
      "[LightGBM] [Info] Number of data points in the train set: 105456, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 71605\n",
      "[LightGBM] [Info] Number of data points in the train set: 120411, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72236\n",
      "[LightGBM] [Info] Number of data points in the train set: 108856, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72710\n",
      "[LightGBM] [Info] Number of data points in the train set: 118035, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 71524\n",
      "[LightGBM] [Info] Number of data points in the train set: 118932, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 68287\n",
      "[LightGBM] [Info] Number of data points in the train set: 116743, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 70498\n",
      "[LightGBM] [Info] Number of data points in the train set: 107520, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 70165\n",
      "[LightGBM] [Info] Number of data points in the train set: 108226, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72211\n",
      "[LightGBM] [Info] Number of data points in the train set: 105960, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76278\n",
      "[LightGBM] [Info] Number of data points in the train set: 112065, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76539\n",
      "[LightGBM] [Info] Number of data points in the train set: 125069, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76118\n",
      "[LightGBM] [Info] Number of data points in the train set: 119722, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 74953\n",
      "[LightGBM] [Info] Number of data points in the train set: 117434, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "皮尔逊相关系数: 0.11454811752056614\n",
      "平均 IC 值: 0.11008040803725297\n",
      "平均r方: 0.0008919785841356581\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./2330/2330_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "\n",
    "window_size = 5\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "# 定义 LightGBM 模型的参数\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'learning_rate': 0.0001,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "# 定义 Adam 优化器\n",
    "optimizer = lgb.LGBMRegressor(boosting_type='gbdt', **params)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "ic_list = []\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "\n",
    "# importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# 对于每个滚动窗口\n",
    "cor = []\n",
    "r_square = []\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1)\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    feature_cols = X_train.columns\n",
    "\n",
    "    # 选择测试数据\n",
    "#         X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "#         X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "#         y_test = X_test['return'].shift(-1).fillna(0)\n",
    "#         X_test = X_test.drop(['return'], axis=1)\n",
    "\n",
    "    # 训练模型\n",
    "    optimizer.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = optimizer.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的皮尔逊相关系数\n",
    "    corr, _ = pearsonr(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r_square.append(r2)\n",
    "    # 将预测结果添加到列表中\n",
    "    ic_list.append(corr)\n",
    "    # fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "    # fi['value'] = optimizer.feature_importances_\n",
    "    # fi = fi[fi.value != 0]\n",
    "    # fi = fi.sort_values(by='value', ascending=False)\n",
    "    # fi = fi.iloc[:10]\n",
    "    # importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "    # importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "# 计算最后一天内所有 tick 预测的平均 IC 值\n",
    "ic = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "print('皮尔逊相关系数:', corr)\n",
    "print('平均 IC 值:', ic)\n",
    "print('平均r方:', np.mean(r_square))\n",
    "joblib.dump(optimizer, '2330_lgbm.pkl')\n",
    "\n",
    "pred_value.to_csv('./lgbm_pred_2330_'+str(m)+'.csv')\n",
    "# importance_name.to_csv('./lgbm_importance_name_'+file[0:5]+file[8:14]+'.csv')\n",
    "# importance_value.to_csv('./lgbm_importance_value_'+file[0:5]+file[8:14]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dba41ee-6049-4796-bed3-d55f1af31628",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./0050/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '0050':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba70ebe0-a34e-4f6c-8f39-cc0bbc6c24ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 121528\n",
      "[LightGBM] [Info] Number of data points in the train set: 126594, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 120395\n",
      "[LightGBM] [Info] Number of data points in the train set: 126240, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116337\n",
      "[LightGBM] [Info] Number of data points in the train set: 121824, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115911\n",
      "[LightGBM] [Info] Number of data points in the train set: 116994, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 121261\n",
      "[LightGBM] [Info] Number of data points in the train set: 139830, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117925\n",
      "[LightGBM] [Info] Number of data points in the train set: 138189, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.351767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 119311\n",
      "[LightGBM] [Info] Number of data points in the train set: 163202, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 121268\n",
      "[LightGBM] [Info] Number of data points in the train set: 158813, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.447234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 118631\n",
      "[LightGBM] [Info] Number of data points in the train set: 155385, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.348249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117651\n",
      "[LightGBM] [Info] Number of data points in the train set: 137280, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115625\n",
      "[LightGBM] [Info] Number of data points in the train set: 124375, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114755\n",
      "[LightGBM] [Info] Number of data points in the train set: 104710, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115222\n",
      "[LightGBM] [Info] Number of data points in the train set: 100457, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116701\n",
      "[LightGBM] [Info] Number of data points in the train set: 106244, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114295\n",
      "[LightGBM] [Info] Number of data points in the train set: 98446, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 113140\n",
      "[LightGBM] [Info] Number of data points in the train set: 103202, number of used features: 710\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "皮尔逊相关系数: 0.12741284374806083\n",
      "平均 IC 值: 0.16160572989728855\n",
      "平均r方: 0.0008890663958662712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./0050/0050_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "\n",
    "window_size = 5\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "# 定义 LightGBM 模型的参数\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'learning_rate': 0.0001,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "# 定义 Adam 优化器\n",
    "optimizer = lgb.LGBMRegressor(boosting_type='gbdt', **params)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "ic_list = []\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "\n",
    "# importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# 对于每个滚动窗口\n",
    "cor = []\n",
    "r_square = []\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1)\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    feature_cols = X_train.columns\n",
    "\n",
    "    # 选择测试数据\n",
    "#         X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "#         X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "#         y_test = X_test['return'].shift(-1).fillna(0)\n",
    "#         X_test = X_test.drop(['return'], axis=1)\n",
    "\n",
    "    # 训练模型\n",
    "    optimizer.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = optimizer.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的皮尔逊相关系数\n",
    "    corr, _ = pearsonr(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r_square.append(r2)\n",
    "    # 将预测结果添加到列表中\n",
    "    ic_list.append(corr)\n",
    "# 计算最后一天内所有 tick 预测的平均 IC 值\n",
    "ic = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "print('皮尔逊相关系数:', corr)\n",
    "print('平均 IC 值:', ic)\n",
    "print('平均r方:', np.mean(r_square))\n",
    "joblib.dump(optimizer, '0050_lgbm.pkl')\n",
    "\n",
    "pred_value.to_csv('./lgbm_pred_0050_'+str(m)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718d69d7-97f3-44be-b964-5993e4ae457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./2603/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '2603':\n",
    "        new_filename.append(filename[i])\n",
    "month = []\n",
    "for file in new_filename:\n",
    "    month.append(int(file[8:14]))\n",
    "month.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80963325-4118-4867-b66d-ed2fc1a3c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n",
      "/tmp/ipykernel_843/414581158.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((avs * aps + bvs * bps).sum(axis=1) / (avs + bvs).sum(axis=1) - df['mid']).shift(lookback_shift).rolling(lookback_len).mean().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 77756\n",
      "[LightGBM] [Info] Number of data points in the train set: 155076, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 77598\n",
      "[LightGBM] [Info] Number of data points in the train set: 156319, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 77488\n",
      "[LightGBM] [Info] Number of data points in the train set: 159323, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76439\n",
      "[LightGBM] [Info] Number of data points in the train set: 140074, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 74152\n",
      "[LightGBM] [Info] Number of data points in the train set: 129870, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 74811\n",
      "[LightGBM] [Info] Number of data points in the train set: 118973, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 77313\n",
      "[LightGBM] [Info] Number of data points in the train set: 124979, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79182\n",
      "[LightGBM] [Info] Number of data points in the train set: 130411, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81163\n",
      "[LightGBM] [Info] Number of data points in the train set: 176338, number of used features: 701\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80723\n",
      "[LightGBM] [Info] Number of data points in the train set: 168262, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.310626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80824\n",
      "[LightGBM] [Info] Number of data points in the train set: 167483, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79763\n",
      "[LightGBM] [Info] Number of data points in the train set: 154778, number of used features: 692\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 77195\n",
      "[LightGBM] [Info] Number of data points in the train set: 137090, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 70678\n",
      "[LightGBM] [Info] Number of data points in the train set: 83854, number of used features: 656\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 67094\n",
      "[LightGBM] [Info] Number of data points in the train set: 79943, number of used features: 548\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66940\n",
      "[LightGBM] [Info] Number of data points in the train set: 75511, number of used features: 638\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "皮尔逊相关系数: 0.539932676848871\n",
      "平均 IC 值: 0.19058377951689537\n",
      "平均r方: 0.0014034321911588071\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = 202308\n",
    "df = pd.read_csv('./2603/2603_md_'+str(m)+'_'+str(m)+'.csv')\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "for i, date in enumerate(date_list):\n",
    "    if i == 0:\n",
    "        df_prc = preprocess(df[df['date'] == date])\n",
    "    else:\n",
    "        df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "for name, (func, *args) in functions.items():\n",
    "    if 'get_ipython' in name:\n",
    "        continue\n",
    "    # print(name)\n",
    "    result = func(df_prc, *args)\n",
    "    var_name = name.replace('get_', '')\n",
    "    df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "\n",
    "window_size = 5\n",
    "date_list = df['date'].unique().tolist()\n",
    "\n",
    "# 定义 LightGBM 模型的参数\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'learning_rate': 0.0001,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "# 定义 Adam 优化器\n",
    "optimizer = lgb.LGBMRegressor(boosting_type='gbdt', **params)\n",
    "\n",
    "# 定义用于存储每个 tick 预测结果的列表\n",
    "ic_list = []\n",
    "pred_value = pd.DataFrame(index=df_prc.index, columns=['date', 'return'])\n",
    "pred_value['date'] = df_prc['date']\n",
    "\n",
    "# importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "# 对于每个滚动窗口\n",
    "cor = []\n",
    "r_square = []\n",
    "for i in range(window_size, len(date_list)):\n",
    "    # 选择训练数据和目标值\n",
    "    X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "    X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_train = X_train['return'].shift(-1).fillna(0)\n",
    "    X_train = X_train.drop(['return'], axis=1)\n",
    "    X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "    X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "    y_test = X_test['return'].shift(-1).fillna(0)\n",
    "    X_test = X_test.drop(['return'], axis=1)\n",
    "    for col in X_train.columns:\n",
    "        if col != 'price':\n",
    "            del X_train[col]\n",
    "            del X_test[col]\n",
    "        if col == 'price':\n",
    "            break\n",
    "    feature_cols = X_train.columns\n",
    "\n",
    "    # 选择测试数据\n",
    "#         X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "#         X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "#         y_test = X_test['return'].shift(-1).fillna(0)\n",
    "#         X_test = X_test.drop(['return'], axis=1)\n",
    "\n",
    "    # 训练模型\n",
    "    optimizer.fit(X_train, y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_pred = optimizer.predict(X_test)\n",
    "    pred_value.loc[X_test.index[0]:X_test.index[-1], 'return'] = y_pred\n",
    "    # 计算预测值和真实值之间的皮尔逊相关系数\n",
    "    corr, _ = pearsonr(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r_square.append(r2)\n",
    "    # 将预测结果添加到列表中\n",
    "    ic_list.append(corr)\n",
    "# 计算最后一天内所有 tick 预测的平均 IC 值\n",
    "ic = np.mean(ic_list)\n",
    "\n",
    "# 输出结果\n",
    "print('皮尔逊相关系数:', corr)\n",
    "print('平均 IC 值:', ic)\n",
    "print('平均r方:', np.mean(r_square))\n",
    "joblib.dump(optimizer, '2603_lgbm.pkl')\n",
    "\n",
    "pred_value.to_csv('./lgbm_pred_2603_'+str(m)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecd0e6-750d-4f7e-b237-a008271183a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir('./2603/')\n",
    "n = len(filename)\n",
    "new_filename = []\n",
    "for i in range(n):\n",
    "    if filename[i][0:4] == '2603':\n",
    "        new_filename.append(filename[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac954f56-99a2-4595-be52-4d3ec3e06f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(new_filename):\n",
    "    df = pd.read_csv('./2603/'+file)\n",
    "    date_list = df['date'].unique().tolist()\n",
    "\n",
    "    for i, date in enumerate(date_list):\n",
    "        if i == 0:\n",
    "            df_prc = preprocess(df[df['date'] == date])\n",
    "        else:\n",
    "            df_prc = pd.concat([df_prc, preprocess(df[df['date'] == date])])\n",
    "    for name, (func, *args) in functions.items():\n",
    "        if 'get_ipython' in name:\n",
    "            continue\n",
    "        # print(name)\n",
    "        result = func(df_prc, *args)\n",
    "        var_name = name.replace('get_', '')\n",
    "        df_prc = pd.concat([df_prc, result.rename(var_name)], axis=1)\n",
    "#     df_prc.to_csv('./factors_2330_'+file[8:14]+'.csv')\n",
    "    \n",
    "    window_size = 5\n",
    "    date_list = df['date'].unique().tolist()\n",
    "\n",
    "    # 定义 LightGBM 模型的参数\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'learning_rate': 0.0001,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'n_estimators': 100,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'silent': True\n",
    "    }\n",
    "\n",
    "    # 定义 Adam 优化器\n",
    "    optimizer = lgb.LGBMRegressor(boosting_type='gbdt', **params)\n",
    "\n",
    "    # 定义用于存储每个 tick 预测结果的列表\n",
    "    ic_list = []\n",
    "    importance_name = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "    importance_value = pd.DataFrame(index=date_list[window_size:], columns=range(10))\n",
    "    # 对于每个滚动窗口\n",
    "    cor = []\n",
    "    r_square = []\n",
    "    for i in range(window_size, len(date_list)):\n",
    "        # 选择训练数据和目标值\n",
    "        X_train = df_prc.loc[(df_prc['date'] < date_list[i]) & (df_prc['date'] >= date_list[i-window_size]), :]\n",
    "        X_train = X_train.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "        y_train = X_train['return'].shift(-1).fillna(0)\n",
    "        X_train = X_train.drop(['return'], axis=1)\n",
    "        X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "        X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "        y_test = X_test['return'].shift(-1).fillna(0)\n",
    "        X_test = X_test.drop(['return'], axis=1)\n",
    "        for col in X_train.columns:\n",
    "            if col != 'price':\n",
    "                del X_train[col]\n",
    "                del X_test[col]\n",
    "            if col == 'price':\n",
    "                break\n",
    "        # for col in X_test.columns:\n",
    "        #     if col != 'price':\n",
    "        #         del X_test[col]\n",
    "        #         # del X_test[col]\n",
    "        #     if col == 'price':\n",
    "        #         break\n",
    "        feature_cols = X_train.columns\n",
    "        \n",
    "\n",
    "        # 选择测试数据\n",
    "#         X_test = df_prc.loc[(df_prc['date'] == date_list[i]), :]\n",
    "#         X_test = X_test.drop(['tick', 'date', 'time'], axis=1)\n",
    "\n",
    "#         y_test = X_test['return'].shift(-1).fillna(0)\n",
    "#         X_test = X_test.drop(['return'], axis=1)\n",
    "\n",
    "        # 训练模型\n",
    "        optimizer.fit(X_train, y_train)\n",
    "\n",
    "        # 进行预测\n",
    "        y_pred = optimizer.predict(X_test)\n",
    "\n",
    "        # 计算预测值和真实值之间的皮尔逊相关系数\n",
    "        corr, _ = pearsonr(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r_square.append(r2)\n",
    "        # 将预测结果添加到列表中\n",
    "        ic_list.append(corr)\n",
    "        fi = pd.DataFrame(index=feature_cols, columns=['value'])\n",
    "        fi['value'] = optimizer.feature_importances_\n",
    "        fi = fi[fi.value != 0]\n",
    "        fi = fi.sort_values(by='value', ascending=False)\n",
    "        fi = fi.iloc[:10]\n",
    "        importance_name.iloc[i-window_size, :len(fi)] = fi.index.tolist()\n",
    "        importance_value.iloc[i-window_size, :len(fi)] = fi['value'].tolist()\n",
    "    # 计算最后一天内所有 tick 预测的平均 IC 值\n",
    "    ic = np.mean(ic_list)\n",
    "\n",
    "    # 输出结果\n",
    "    print('皮尔逊相关系数:', corr)\n",
    "    print('平均 IC 值:', ic)\n",
    "    print('平均r方:', np.mean(r_square))\n",
    "\n",
    "     \n",
    "    importance_name.to_csv('./lgbm_importance_name_'+file[0:5]+file[8:14]+'.csv')\n",
    "    importance_value.to_csv('./lgbm_importance_value_'+file[0:5]+file[8:14]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4da87-31eb-42d2-884f-042048dc4b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
